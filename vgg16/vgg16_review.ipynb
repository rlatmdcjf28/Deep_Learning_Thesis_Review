{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f586888",
   "metadata": {},
   "source": [
    "# 🚞 0. ABSTRACT - 초록\n",
    "\n",
    "- ### 이 논문은 large-scale image recognition setting에서 CNN의 깊이(depth)가 정확도에 미치는 영향을 연구한다.<br/>\n",
    "\n",
    "- ### 저자들은 3X3 Convolution Filter가 있는 구조를 사용하여 깊이가 증가하는 네트워크를 철저하게 평가하고<br/><br/>이 Filter를 깊이가 16 ~ 19인 가중치 layer에 넣어 이전의 구성보다 발전된 결과를 달성하였음을 보여준다.<br/>\n",
    "\n",
    "- ### 위 결과는 2014년의 ImageNet Challenge 2014 submission의 기초가 되었으며,<br/><br/>이들은 localisation과 classification tracks 에서 각각 1위와 2위를 차지하였다.<br/>\n",
    "\n",
    "- ### 저자들은 그들의 representation이 다른 dataset에도 잘 일반화되어 최고수준의 결과를 달성한다는 것을 설명한다.<br/>\n",
    "\n",
    "- ### 또, 저자들은 Computer Vision의 Deep visual representations에 대한 추가적인 연구를 용이하게 하기 위해<br/><br/>가장 성능이 좋은 두 개의 ConvNet 모델을 공개적으로 사용할 수 있도록 하였다.<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de46fc1",
   "metadata": {},
   "source": [
    "# 🚞 1. INTRODUCTION - 소개\n",
    "- ### 이 부분에서, 저자들은 기존의 ConvNet에 대해서 설명하고, 기존의 ConvNet을 개선한 architecture들을 소개한다.<br/><br/><span style=\"color: gray\">기존의 방법 -> [smaller receptive field와 smaller stride를 사용한 ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014)  &&   image와 multiple scale에 걸쳐 빽빽하게 네트워크를 훈련하고 테스트 (Sermanet et al., 2014; Howard, 2014)]</span>\n",
    "\n",
    "- ### 그리고 이 논문에서는 ConvNet Architecture 설계의 또 다른 중요한 측면인 <span style='background-color:#fff5b1'>깊이(Depth)</span>를 다룰것이라 소개한다.\n",
    "\n",
    "- ### 이를 위해 Architecture의 다른 parameter를 수정하고 Convolution layer를 사용하여<br/><br/> 네트워크의 깊이를 꾸준히 증가시키는데,이것은 <span style='background-color:#fff5b1'>모든 layer에서 작은 크기의 (3X3) Convolution filter를 사용하기 때문에 가능</span>하다.\n",
    "\n",
    "- ### 결과적으로 저자들은 ILSVRC classification 와 localisation 작업에 대해 최고 수준의 정확도를 달성할 뿐만 아니라 훨씬 더 정확한 ConvNet을 고안하였다.<br/><br/>이 ConvNet은 다른 이미지 dataset에 적용할 수 있으며 비교적 단순한 파이프라인의 일부로 사용하더라도 우수한 성능을 보인다고 한다.\n",
    "\n",
    "- ### 그리고 논문의 나머지 구성에 대해 얘기하는데, 2장은 ConvNet 구성에 대해 설명하고, 3장은 이미지 분류 및 평가에 대한 자세한 내용, <br/><br/>5장은 논문을 마무리하는 부분으로 구성되어 있다.\n",
    "\n",
    "- ### 추가적으로, 완전성을 위해 ILSVRC-2014 object localisation 시스템을 Appendix A 에서 설명하고, <br/><br/>다른 dataset에 대한 매우 심층적인 기능의 일반화에 대해서도 설명한다고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b592c",
   "metadata": {},
   "source": [
    "# 🚞 2. CONVNET CONFIGURATIONS - ConvNet 구성<br/>\n",
    "- ##### 공정한 환경에서의 실험을 위해, 저자들은 ConvNet 깊이의 증가로 인한 개선을 측정하기 위해 모든 ConvNet layer의 구성은 <br/><br/>Ciresan et al.(2011); Krizhevsky et al. (2012) 에서 영감을 받아 동일한 원칙을 사용하여 설계하였다.\n",
    "- ##### 이 section에서는 먼저 ConvNet 구성의 일반적인 layout을 설명한 다음, 평가에 사용된 특정 구성을 자세히 설명한다. <br/><br/>그런 다음 저자들의 design choice에 대해 논의하고 이전의 기술과 비교한다.<br/><br/>\n",
    "\n",
    "\n",
    "+ ## 🛤 2.1 ARCHITECTURE - 구조\n",
    "    - ### <span style='background-color:#fff5b1'>ConvNet에 대한 Input은 고정된 크기인 (224X224) pixel의 RGB 이미지</span>이다.  \n",
    "    \n",
    "    - ### 전처리 과정으로는 각 픽셀에서 Train세트를 기반으로 계산된 평균 RGB 값을 빼는 것만 수행한다.  \n",
    "    - ### image는 Convolution layer의 stack을 통해 전달되며, <span style='background-color:#fff5b1'>저자는 매우 작은 receptive field(3X3)을 가진 filter를 사용</span>한다. <br/><br/>한 가지 구성으로, (1X1) Conv filter를 사용하여 입력 채널의 linear transformation으로 볼 수 있다. \n",
    "    \n",
    "    - ### <span style='background-color:#fff5b1'>stride는 1로 고정</span>되어 있으며 Convolution layer input의 spatial padding은 convolution 연산 후 spatial resolution이 보존되도록 조정된다. <br/><br/>즉, (3X3) Convolution layer에 대해 padding은 1 pixel이다.\n",
    "    \n",
    "    - ### spatial pooling은 일부 Conv layer 뒤에 따라오는 <span style='background-color:#fff5b1'>5개의 MaxPooling layer</span>에 의해 수행된다.<br/><br/>(모든 Conv layer가 MaxPooling을 따르는 것은 아님)\n",
    "    \n",
    "    - ### MaxPooling은 <span style='background-color:#fff5b1'>(2X2) pixel window</span>를 사용.\n",
    "    \n",
    "    - ### Convolutional layer의 stack 다음에는 <span style='background-color:#fff5b1'>세 개의 Fully-Connected 레이어</span>가 따른다. <br/><br/>첫번째, 두번째 FC layer는 각각 4096개의 채널을 가지고, <br/><br/>세번째 FC 레이어는 1000개의 채널을 가지며 1000개의 클래스에 대한 <br/><br/>ILSVRC (ImageNet Large Scale Visual Recognition Challenge) 분류를 수행한다.\n",
    "    \n",
    "    - ### 마지막 layer는 <span style='background-color:#fff5b1'>softmax layer</span>이다. Fully Connected layer의 구성은 모든 network에서 동일하다.\n",
    "    - ### 모든 hidden layer에는 <span style='background-color:#fff5b1'>ReLU</span> (ReLU (Krizhevsky et al., 2012)) 비선형 함수가 적용된다.\n",
    "    - ### 네트워크 중 하나를 제외한 모든 네트워크는 Local Response Normalisation (LRN) 정규화 (Krizhevsky et al., 2012)를 사용하지 않는다.<br/><br/>ILSVRC 데이터셋의 성능 향상에는 도움이 되지 않으며, 메모리 사용량과 계산 시간이 증가하는 경향이 있다고 한다. \n",
    "                \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4117102",
   "metadata": {},
   "source": [
    "+ ## 🛤 2.2 CONFIGURATIONS - 구성\n",
    "    - ### 이 논문에서 평가된 ConvNet 구성은 각각 테이블 1에 개별적으로 설명되어 있다. <br/><br/> 이후에는 각 네트워크를 이름(A-E)으로 참조한다.<br/><br/>  모든 구성은 2.1절에서 제시한 일반적인 디자인을 따르며, 깊이만 다르다. <br/><br/>네트워크 A에서는 11개의 가중치 레이어(8개의 Conv layer와 3개의 FC layer)가 있고, <br/><br/>네트워크 E에서는 19개의 가중치 레이어(16개의 Conv layer와 3개의 FC layer)가 있다.<br/><br/>Conv layer의 너비(채널 수)는 상당히 작으며, 첫번째 layer에서 64에서 시작하여 MaxPooling layer마다 2배씩 증가하고, 512까지 도달한다.<br/>\n",
    "    <span style=\"color: gray\">모델 구성 그림은 summary 참조</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48ef08",
   "metadata": {},
   "source": [
    "+ ## 🛤 2.3 DISCUSSION - 논의\n",
    "    - ### <span style=\"color: gray\">이 부분에서, 저자들은 ILSVRC-2012 (Krizhevsky et al., 2012) 및 ILSVRC-2013 대회 (Zeiler & Fergus, 2013; Sermanet et al., 2014)에서 사용된 <br/><br/>ConvNet과 자신들의 ConvNet의 구성 차이를 설명한다.</span>\n",
    "    \n",
    "    - ### 저자들은 기존의 방법인 11×11 with stride 4 in (Krizhevsky et al., 2012), 또는 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)<br/><br/>처럼 큰 receptive field를 사용하는 대신에, <span style='background-color:#fff5b1'>매우 작은 (3×3) receptive field를 사용</span>한다.(stride 1).\n",
    "    \n",
    "    - ### <span style='background-color:#fff5b1'>single 7×7 layer 대신에 3개의 3×3 Conv layer stack을 사용함으로써 얻는 이점이 있다.</span>\n",
    "    \n",
    "    - ### <span style='background-color:#fff5b1'>첫째</span>, single layer 대신에 3개의 non-linear layers를 통합하여 모델이 더 잘 구분하도록 만든다.\n",
    "    \n",
    "    - ### <span style='background-color:#fff5b1'>둘째</span>, parameter 수를 줄인다. 예를들어, 입력과 출력이 C개의 channel을 가진 3개의 3×3 conv stack의 경우, <br/><br/>이 stack은 $3 \\times (3^2 × C^2) = 27\\times C^2$개의 가중치로 parametrise된다.<br/><br/>한편, single 7$\\times$7 convolution layer는 $7^2 \\times C^2 = 49\\times C^2$개의 parameter가 필요하므로, <br/><br/> 전자보다 약 81%정도 더 많다.\n",
    "    \n",
    "    - ### <span style='background-color:#fff5b1'>1$\\times$1 Conv layer(Configuration C, Table 1)</span>의 결합은 Conv layer의 receptive field에 영향을 주지 않으면서 <br/><br/> dicision function의 non-linearlity를 증가시키는 방법이다.<br/><br/>1$\\times$1 convolution이 본질적으로 동일한 dimensionality의 공간에 대한 linear projection이지만, <br/><br/><span style='background-color:#fff5b1'>rectification function에 의해 추가적인 non-linearlity가 도입</span>된다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e637969",
   "metadata": {},
   "source": [
    "# 🚞 3. CLASSIFICATION FRAMEWORK<br/>\n",
    "\n",
    "- ##### 이전 section에서는 Network의 구성에 대해 자세하게 설명하였고, <br/><br/>이 section에서는 classification ConvNet training과 evaluation에 대해 자세하게 설명 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ebdbb",
   "metadata": {},
   "source": [
    "+ ## 🛤 3.1 Training - 훈련\n",
    "    - ### ConvNet 훈련 절차는 일반적으로 Krizhevsky et al.(2012)를 따른다.<br/><span style=\"color: gray\">(나중에 설명하겠지만, multi-scale training images에서 input crops를 샘플링 하는 것은 제외)</span><br/>\n",
    "    \n",
    "    - ### Optimization : momentum이 있는 backpropagation 기반의 mini-batch gradient descent(LeCun et al., 1989)<br/><br/>를 사용하여  multinomial logistic regression objective(momentum = 0.9, batch_size = 256)<br/>\n",
    "    \n",
    "    - ### Regularization : weight decay(L2 norm이 5 $\\times$ 10$^{-4}$)<br/>\n",
    "    \n",
    "    - ### Dropout : 처음의 두 fully connected layers에 대한 Dropout(rate=0.5)<br/>\n",
    "    \n",
    "    - ### Learning Rate : 처음에 0.01로 설정되었다가, 그 후에 validation accuracy가 향상되지 않을 때마다 10배씩 감소되었다.<br/><br/>전체 학습률은 3배 감소하였으며(0.00001), 370,000의 반복(74 epochs) 후에 중지되었다.<br/>\n",
    "    \n",
    "    - ### 저자들은 그들의 네트워크가 (Krizhevsky et al., 2012)와 비교했을 때 더 많은 parameter와 네트워크가 더 깊음에도 불구<br/><br/>하고, 신경망이 수렴하기위해 더 적은 epoch가 필요하다고 했는데,그 이유로 2가지가 있다. <br/><br/>(a) 깊이가 더 깊고 작은 conv filter 크기로 인해 가해지는 내재적인(암묵적인?) 정규화<br/><br/>(b) 특정 레이어에 대한 pre-initialization<br/>\n",
    "    \n",
    "    - ### 네트워크의 가중치 초기화는 중요하다. 왜냐하면 깊은 신경망에서 기울기의 불안정성으로 인해 초기화가 잘못되면 <br/><br/>학습이 정체될 수 있기 때문이다.<br/><br/> 이 문제를 극복하기 위해 저자들은 Network의 초반에 random initialization로 충분한 성능을 낼 수 있는 얕은 네트워크<br/><Br/>Configuration A를 먼저 학습시켰다. <br/><br/>그런 다음, 더 깊은 다른 Network를 훈련 할 때에는 처음 4개의 Conv layer와 마지막 3개의 Fully-connected layer를 <br/><br/>A로부터 가져와 초기화하고, 나머지 중간 layer는 무작위로 초기화하였다.<br/><br/>Learning Rate의 경우 위에서 언급한 대로 학습하는 중에 줄어들게 설정하였다.<br/><br/> 중간 layer에서 무작위로 초기화 할 때는 $N(0, 0.01)$에서 가중치를 샘플링 하였다.<br/><br/> 또한, bias=0으로 설정하였다.<br/>\n",
    "    \n",
    "    - ### 논문 발표 이후, 저자들은 가중치 초기화에 많이 쓰이고 있는 Xavier initialization을 사용할 수도 있다는 점을 발견하였다.<br/><br/>\n",
    "    \n",
    "    - ### 고정된 입력 이미지의 크기(224$\\times$224)를 얻기 위해, Train image에서 무작위로 crop하였다.<br/><br/>또, Image Augmentation을 위해 잘라진 이미지에 대해 Random Horizontal Flipping과<br/><br/> Random RGB Color Shift를 적용하였다. <br/><br/>\n",
    "    \n",
    "    - ### 또, 이미지 사이즈의 경우 256 ~ 512의 범위에서 이미지의 크기를 random하게 sampling하고 <br/><br/>그 sampling한 image에서 이미지를 crop하여 사용하였다.<br/><br/>이 때 이미지의 크기를 크게 sampling 할 경우, crop할 부분은 객체의 일부 혹은 작은 객체를 포함한 이미지의 작은 부분에<br/><br/> 해당하고, 이미지의 크기를 낮게 sampling 할 경우 crop할 부분은 전체 이미지에 대한 특징을 담고 있다.<br/><br/>\n",
    "    \n",
    "\n",
    "+ ## 🛤 3.2 Testing - 테스트\n",
    "    - ### 리뷰하기 좀 어렵...이해가 잘 안됨(나중에 다시 시도)<br/><br/>\n",
    " \n",
    "+ ## 🛤 3.3 Implimentation details - 실험 세부사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6618618",
   "metadata": {},
   "source": [
    "# 🚞  4. CLASSIFICATION EXPERIMENTS<br/>\n",
    "\n",
    "- ##### Dataset -> ILSVRC-2012(1000 classes), training(1.3M images), validation(50K images), testing(100K images)\n",
    "+ ##### 두 가지 방법을 이용해서 classification performance를 평가한다.\n",
    "+ ##### 첫번째로 top-1이라고 하는 방법은 multi-class classification error로, 잘못 분류된 이미지의 비율을 의미한다.\n",
    "+ ##### 두번째로 top-5는 ILSVRC에서 주로 사용하는 평가 기준으로,<br/><br/>실제 category가 상위 5개의 predicted category에 포함하지 않는 이미지의 비율로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f5984",
   "metadata": {},
   "source": [
    "+ ## 🛤 4.1 Single-Scale Evaluation\n",
    "    + ### test image의 size는 다음과 같이 설정되었다.<br/>\n",
    "    \n",
    "    + ### $Q = S$, $S$는 고정된 값이고 $Q = 0.5(S_{min} + S_{max})$ for jittered $S∈[S_{min}, S_{max}]$<br/><br/> 이 결과는 'Table 3'에 나와있다.<br/>\n",
    "    \n",
    "    + ### 먼저 local response normalization(A-LRN network)은 normalization layer 없이 model A와 비교하여<br/><br/>개선되지 않는다는 사실을 알 수 있다.<br/><br/> 따라서 우리는 더 깊은 architectures(B ~ E)에서 normalization을 사용하지 않는다.<br/>\n",
    "    \n",
    "    + ### 두번째로, 우리는 ConvNet 깊이가 증가됨에따라, classification error가 감소하는 것을 관찰할 수 있다.<br/><br/> <span style=\"color: gray\">(11개의 layer가 있는 A와 19개의 layer가 있는 E를 비교)</span><br/><br/>특히, 같은 depth를 가짐에도 불구하고, 1$\\times$1 conv layer를 포함하는 C는 <br/><br/>네트워크 전반에 걸쳐 3$\\times$3 conv layer를 사용하는 D보다 성능이 떨어진다.<br/><br/> 이는 추가적인 non-linearlity는 도움이 되지만(C가 B보다 성능이 좋음) non-trivial한 receptive field를 가진 <br/><br/> conv filter를 사용하여 spatial context를 포착하는 것이 중요하다는 것을 알려준다.(D가 C보다 성능이 좋음)<br/>\n",
    "    \n",
    "    + ### 마지막으로, Training 할 때에 scale jittering$(S ∈ [256; 512])$을 주는 것은 <br/><br/> 고정된 작은 side를 가진 이미지$(S = 256　or　S = 384)$로 Train하는 것 보다 훨씬 더 좋을 결과를 가져온다. <br/><br/> 비록 Test 할 때는 single scale을 사용하지만, scale jettering에 의한 augmentation이 <br/><br/> multi-scale image statistics를 캡처하는데 도움이 된다는 것을 알 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074d88f",
   "metadata": {},
   "source": [
    "+ ## 🛤 4.2 Multi-Scale Evaluation\n",
    "    + ### single scale에서 ConvNet 모델을 평가한 후에는 test에서 scale jettering의 효과를 평가한다.<br/>\n",
    "    \n",
    "    + ### 이는 test image의 다른 Q값에서 모델을 실행한 다음 <br/><br/> 결과적으로 얻은 class posteriors을 averaging하는 것으로 구성된다. <br/><br/> \n",
    "    \n",
    "    + ### Train과 Test 사이의 큰 scale 차이는 성능 하락을 가져오므로, 고정된 $S$로 훈련된 모델은 <br/><br/> 훈련과 유사한 세 개의 Test image 크기로 평가되었다. <br/><br/> <span style=\"color: gray\">$(Q = \\left\\{S-32,　S,　S+32\\right\\})$</span> <br/><br/>\n",
    "    \n",
    "    + ### 동시에, 훈련 할 때 scale jettering은 ConvNet을 테스트 할 때 더 넓은 범위의 scale에 적용할 수 있도록 하므로 <br/><br/> $S∈[S_{min},　S_{max}]$로 훈련된 모델은 더 큰 범위의 크기 $Q = \\left\\{ S_{min},　0.5(S_{min}+S_{max}), 　S_{max} \\right\\}$<br/><br/> 로 평가되었다.<br/><br/>\n",
    "    \n",
    "    + ### Table 4에 나타난 결과는 Test 할 때의 scale jettering이 single scale에서 same model을 평가하는 것과 비교하여 <br/><br/> 더 좋은 성능을 보여준다. <br/><br/> 이전과 마찬가지로, 가장 깊은 구성(D와 E)이 가장 우수한 성능을 발휘하며, <br/><br/> scale jettering이 고정된 가장 작은 side를 사용하여 훈련하는 것보다 우수한 성능을 보여준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92e62d",
   "metadata": {},
   "source": [
    "+ ## 🛤 4.3 Multi-Crop Evaluation\n",
    "    + ### Table 5는 multi-crop evaluation과 Dense ConvNet evaluation을 비교한다. <br/><br/> 또한, 두 가지 평가기법의 보완성(?)을 그들의 softmax output의 average를 통해 평가한다. <br/><br/> 보다시피, multiple crop은 dense evaluation보다 약간 더 우수한 성능을 발휘하며, <br/><br/> 실제로 두 방식은 상호보완적이다. <br/><br/> 위에서 언급했듯이, 이들을 결합한 것이 우수한 결과를 나타낸다. <br/><br/> 이들을 결합한 것이 우수한 결과를 내는 이유로 <br/><br/> 저자들은  convolution boundary conditions의 다른 처리 때문이라고 가정하였다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393685a",
   "metadata": {},
   "source": [
    "+ ## 🛤 4.4 ConvNet Fusion\n",
    "    + ### 결과는 Table 6 참조.<br/>\n",
    "    + ### 지금까지는 개별적인 ConvNet model의 성능을 평가해왔다. <br/><br/> 이 부분의 실험에서는 several model의 출력을 averaging하여 combine한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77115fa",
   "metadata": {},
   "source": [
    "+ ## 🛤 4.5 Comparison with The State Of The Art\n",
    "    + ### 마지막으로, 저자들은 자신들의 결과를 최첨단 기술과 비교한다.<br/>\n",
    "    + ### Table 7 참조."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d0a18",
   "metadata": {},
   "source": [
    "# 🚞 CONCLUSION\n",
    "\n",
    "+ ### 이 연구에서, 저자들은 large-scale image classification을 위해 <br/><br/> 매우 깊은 Convolutional Network(최대 19개의 weight layers)를 평가하였다.\n",
    "\n",
    "+ ### representation depth가 classification accuracy에 유익하고, 깊이가 크게 증가한 ConvNet architecture를 사용하여 <br/><br/> ImageNet Challenge Dataset에서 우수한 성능을 달성할 수 있음을 입증하였다.\n",
    "\n",
    "+ ### 저자들의 결과로 시각적 표현에서의 depth의 중요성을 다시 확인할 수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b632a0b",
   "metadata": {},
   "source": [
    "# 🚌 LOCALIZATION - Object Localization\n",
    "+ ##### 이 section은 ILSVRC Challenge의 classification task에서 localization task으로 전환한다.\n",
    "+ ##### 저자들의 방법은 A.1절에서 설명되고, A.2절에서 평가되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb5b2e",
   "metadata": {},
   "source": [
    "+ ## 🛤 A.1 Localization ConvNet\n",
    "    + ### Object Localization을 수행하기 위해 매우 깊은 ConvNet을 사용한다. <br/><br/> 여기서는 마지막의 Fully-Connected layer가 class 점수 대신에 bounding box의 위치를 예측한다.<br/>\n",
    "    \n",
    "    + ### Bounding Box는 중심좌표, 너비 및 높이를 저장하는 4-D Vector로 표현된다.<br/><br/> Bounding Box prediction은 모든 클래스에 대해 공유되는지, 또는 클래스별로 구분되는지에 따라 선택할 수 있다. <br/><br/> 전자의 경우에 마지막 층은 4-D이며, 후자의 경우 4000-D이다.(1000개의 class) <br/><br/> 마지막 Bounding Box prediction layer를 제외하고는 classification task에서 최고 성능을 발휘한 <br/><br/> Table 1의 ConvNet architecture D를 사용한다.<br/>\n",
    "    \n",
    "+ ## 🛤 A.1.1 Training \n",
    "    + ### Localization ConvNet의 Training은 classification ConvNet과 유사하다.<br/><br/> 주요 차이점은 logistic regression object를 실제 Bounding box parameter와의 차이를 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26884e45",
   "metadata": {},
   "source": [
    "# 🚌 GENERALISATION OF VERY DEEP FEATURES\n",
    "\n",
    "# 🚌 PAPER REVISIONS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
